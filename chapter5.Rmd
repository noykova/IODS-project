# 5. Dimensionality reduction technique

##5.1. The data 

In this chapter we use **human** data from the United Nations Development Programme, which are originally published [here](http://hdr.undp.org/en/content/human-development-index-hdi). The data concern so called human development index (HDI) as a criteria for the development of a country. HDI is a summary measure of three main groups of variables about long and healthy life, being knowledgeable and have a decent standard of living.
Original data include 195 observations of 19 variables. The meaning of all variables is described [here](https://raw.githubusercontent.com/TuomoNieminen/Helsinki-Open-Data-Science/master/datasets/human_meta.txt). 


The wrangling includes the following changes: 

1. Two new columns are added to "Gender inequality" data: the ratio of Female and Male populations with secondary education in each country. (i.e. edu2F / edu2M), and the second one - the ratio of labour force participation of females and males in each country (i.e. labF / labM). 

2. Next "Gender inequality" and "Human development" datas are joined using the variable Country as the identifier. 

3. The variable GNi is transformed to numeric. Only the columns c("Country", "Edu2.FM", "Labo.FM", "Edu.Exp", "Life.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F") are selected for further work. 

4. The rows with missing values are removed. The last 10 observations relate to regions instead of countries. Therefore they are removed. 

5. Finally the country names are added as rownames and the corresponding ciolumn is removed from data frame. 

The transformed data involve 155 observations of 8 variables. 

##5.2.Graphical overview of the data and summaries of the variables 

The preliminary treated data look as: 

```{r}
human= read.csv("human.csv") 
str(human)
head(human)
```


Next we use ggpairs() for creating advanced plot matrix. 

```{r}
library(dplyr)
library(corrplot)
library(GGally)
human_ <- dplyr::select(human, -X)
ggpairs(human_, diag=list(continuous="density", discrete="bar"), axisLabels="show")
```

We see that the only variable which has near to normal distribution is Edu.Exp. It is also highly correlated to almost all other variables. Edu2.FM also shows correlation to the other variables, but the correlation coefficients are smaller comparing to coreesponding correlation coefficients of Edu.Exp. Life.Exp is positevily correlated to GNI, and negatively to Mat.Mor and Ado.Birth. GNI and Math.Mor are also negatively correlated. Math.mor has strong positive correlaton with Ado.Birth. 

More clearly these dependences are shown using corrplot():
```{r}
cor_matrix<-cor(human_)%>% round(digits =2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```

##5.3.Principal component analysis (PCA) on non- standardized and standardized data.

In this chapter we explore two methods for unsupervised learning - Principal component analysis and correspondence analysis. In the unsupervised learning we have only a set of features X1,X2, . . . , Xp measured on n observations. We do not have an associated response variable Y. Therefore here we are not interesting in the prediction. The goal is to discover interesting relations among the measurements on X1,X2, . . .,Xp. PCA is a tool used for data visualization or data pre-processing before applying methods for supervised learning. Here we apply this technique for dimensionality reduction. It is performed by singular value matrix decomposition method known from linear algebra. The data is transformed to new space with equal or less number of dimensions, called principal components. Principal components are expressed by relationships involving the original features X1,X2, . . .,Xp. Principal components are uncorrelated and each is less important than the previous one in terms of captured variance.


###5.3.1 Non standardized data

First we perform PCA on the not standardized human data.
```{r}
pca_human <- prcomp(human_)
```

The results of PCA are visualized using biplots. These are scatter plots where observations are placed on x and y coordinates defined by first two principal components PC1 and PC2. Arrows and labels on these plots represent the connections between original features and principal components. The length of the arrows are proportional to the standard deviations of X1,X2, . . .,Xp. The angle between a feature and a PC axis corresponds to correlation of its arrows. Small angle corresponds to high positive correlation. Analogously the angle between arrows corresponds to the correlation between two features.

The biplot of PCA on non-standardized human datais obtain using biplot() function.
```{r}
biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

The points marked in grey are the number of observations, which corresponds to the different countries, while arrows show the standard deviation of the corresponding feature.

It is difficult to analize this plot because the different dimensions of the investigated features. We observed the highest impact on PC1 has the variable GNI. This highest correlation is negative.


###5.3.2 Standardized data

Next we apply standardization to X1,X2, . . .,Xp.
```{r}
human_std <- scale(human_)
summary(human_std)
```

The function scale()=(x-mean(x))/sd(x). Therefore the mean of standardized data is 0, and their standard deviation is 1.

In contrast with linear regression, in which scaling the variables has no effect, here scaling change results.

In linear regression multiplying a variable by a factor of c will lead to multiplication of the corresponding coefficient estimate by a factor of 1/c.

In PCA the situation is different because the different scaling factors will directly influence calculation of each PC components. As we have observed on biplot above, when dimensions are different, the absolute values of variances also differ a lot. Then the first principal component loading vector have a very large loading for GNI, since this variable has the highest variance.

We provide PCA using scaled data.
```{r}
pca_human <- prcomp(human_std)
biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

After comparing last two biplots we see that the scaling has substantial effect on the results. Because it is undesirable for the principal components obtained to depend on an arbitrary choice of scaling, we typically scale each variable to have standard deviation one before we perform PCA.

The results of last biplot shows that Mat.Mor is highly correlated to PC1, while Parli.F is highly correlated to PC2. We can assume that correlation between these original features is negligible. So, we conclude that PC1 places most of its weight on Mat.Mor, while PC2 places most of its weight on Parli.F. We observe also high correlation between Mat.Mor, Ado.Birth, Edu.Exp, Edu2.FM, Life.Exp and GNi, which was also shown on correlation matrix plot. Here we see that the observations on right hand side of this biplot involve countries, where the maternal mortality ratio and adolescent birth rate are high, but (since the negative correlation) in the same time the Gross National Income per capita, expected years of schooling, the ratio between females and males with at least secondary education, and life expectancy at birth are low. On the left hand side of this biplot we see the opposite situation - the countries where the maternal mortality ratio and adolescent birth rate are low, and in the same time the Gross National Income per capita, expected years of schooling, the ratio between females and males with at least secondary education, and life expectancy at birth are high. On the upper part of this plot we see the countries where the proportion of female and male on the labour market is high, and also the percentage of female presentation in the parliament is high. The opposite situation is observed on the bottom part of this biplot. Therefore we can conclude that the best situation where we expect high human development index is in the countries, which are situated on the left upper side of this biplot.

We take a look on summary of PCA:
```{r}
s <- summary(pca_human)
s
```
and round the percetange of variance captured by each PC.
```{r}
pca_pr <- round(1*s$importance[2, ]*100, digits = 1)
pca_pr
```

Finally we add these percetanges of variance on the biplot
```{r}
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```


With this new information we have answer to the question how much of the information in human data set is lost by projecting the observations onto the first two principal components? The amount of variance, which is not contained in PC1 + PC2 = 100 - (53,6 + 16,2) = 30,2 %.


##5.4 Multiple Correspondence Analysis

In the case of categorical variables dimensionality reduction can be provided using correspondence analysis (CA, n=2 variables) or multiple corresponding analysis (MCA, n>2). MCA is a generalization of PCA and an extension of CA. In this analysis cross tabulation matrices between all variables in the data set are used. The information from cross-tabulations has to be presented in clear graphical form. MCA could be used for example as a pre-processing for clustering.


###5.4.1 The data

Here we use tea data from FactoMineR package. We use 6 categorical variables from the whole dataset.
```{r}
library(FactoMineR)
library(tidyr)
library(ggplot2)
library(dplyr)
data("tea")
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- dplyr::select(tea, one_of(keep_columns))
summary(tea_time)
```

```{r}
str(tea_time)
```

Only variables sugar and lunch are binomial, others express 3 (Tea,how,where) or 4 (How) levels. Therefore MCA approach is feasible for investigating these data.

Graphically the date are presented as barplots using ggplot() function.
```{r}
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) 
```
The amount of people, who use to drink their tea with sugar is almost the same as the others, who do not put sugar in their tea. Other variables have different number of observations for the different levels. We observe that there are not variable categories with a very low frequency, which could distort MCA analysis.

Next MCA analysis on the chosen tea data is provided.
```{r}
mca <- MCA(tea_time, graph = FALSE)
```
In a correspondence analysis, a crosstabulation table of frequencies is first standardized, so that the relative frequencies across all cells sum to 1.0. The goal of (M)CA is to represent the entries in the table of relative frequencies in terms of the distances between individual rows and/or columns in a low-dimensional space. In the terminology of (M)CA, the row and column totals of the matrix of relative frequencies are called the row mass and column mass.
```{r}
summary(mca)
```
MCA results is interpreted as the results from a simple CA.

The first table contains the variances and the percentage of variances retained by each dimension. Eigenvalues correspond to the amount of information retained by each axis. Dimensions are ordered decreasingly and listed according to the amount of variance explained in the solution. Dimension 1 explains the most variance in the solution, followed by dimension 2 till Dimension 11.

The resulting Table 2 in summary() function contains the coordinates, the contribution and the cos2 (quality of representation [in 0-1]) of the first 10 active individuals on the dimensions 1 and 2.

Table 3 in summary() contains the coordinates, the contribution and the cos2 (quality of representation [in 0-1]) of the first 10 active variable categories on the dimensions 1 and 2. This table contains also a column called v.test. The value of the v.test is generally comprised between 2 and -2. For a given variable category, if the absolute value of the v.test is superior to 2, this means that the coordinate is significantly different from 0. This is a case for almost all variables involved in Dim.1, except the categories alone and other. The absolute value of v.test is less then 2 for category black in Dim.2, and for 5 categories in Dim.3 - green, milk, tea bag, tea bag + unpackaged and unpackaged.

Table 4 in summary() is categorical variables (eta2): contains the squared correlation between each variable and the dimensions. If the value is close to 1, it indicates a strong link with the variable and dimension. In Dim.1 this is a case for the variables how and where. In other words, from this table we see that the variables how and where are strongly linked to the first and a little bit less to the second dimension, and the variables Tea, How, suger and lunch are more related to the third dimension.

We first see from Table 4 which variables are linked to which dimensions, and then from Table 3 we observe the categories that are linked to the dimensions.

For further clarifying the MCA results we use graphical representation. MCA scatter plot draws a biplot of individuals and variable categories.
```{r}
plot(mca, invisible=c("ind"), habillage = "quali")
```

MCA factor map is a two dimensional biplot of the active categorical levels. On this plot the distances between levels and the barycenter, and the distance between levels are calculated and shown. These distances provide information about similarity of different categories.

On this biplot only first two dimensions are shown. We observe that the categories Earl Grey, sugar and milk are close to each other, so people who prefer to drink Earl Gray most probably we do it with sugar and milk. There is similarity between unpackaged and tea shop categories, and also between tea bag + unpackaged and chain store + tea shop categories. The category green is alone quite far from the other categories.

##Useful links

[PCA explained visually](http://setosa.io/ev/principal-component-analysis/)

[Mathematical tutorial on PCA](https://arxiv.org/pdf/1404.1100.pdf)

[Biplots in practise](http://www.multivariatestatistics.org/biplots.html)

[Correspondence Analysis](https://documents.software.dell.com/statistics/textbook/correspondence-analysis#general)

[The FactoMineR webpage](http://factominer.free.fr/)

[Filter data with dplyr](https://blog.exploratory.io/filter-data-with-dplyr-76cf5f1a258e#.o7x9uvea7)

[Multiple Correspondence Analysis Essentials: Interpretation and application to investigate the associations between categories of multiple qualitative variables - R software and data mining](http://www.sthda.com/english/wiki/multiple-correspondence-analysis-essentials-interpretation-and-application-to-investigate-the-associations-between-categories-of-multiple-qualitative-variables-r-software-and-data-mining)


##Link to to the GitHub page, related to my work on this course:

https://github.com/noykova/IODS-project
